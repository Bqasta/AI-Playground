llamaCppConfig = {
    "ggufLLM": "../service/models/llm/ggufLLM",
}

device = "xpu"
